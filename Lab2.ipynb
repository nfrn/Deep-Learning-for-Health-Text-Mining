{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "pip install keras-tqdm\n",
    "pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset AnEM:\n",
    "### Description:\n",
    "\"Anatomical entities such as kidney, muscle and blood are central to much of biomedical scientific discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts.\" \"The corpus consists of 500 documents (over 90,000 words) selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature. The corpus annotation covers mentions of both healthy and pathological anatomical entities and contains over 3,000 annotated mentions.\" http://www.nactem.ac.uk/anatomy/ Paper: http://www.nactem.ac.uk/anatomy/docs/ohta2012opendomain.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Visualization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1\" style='display:inline'><caption>Entities Distribution</caption> \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Tokens</th> \n",
       "        <th class=\"col_heading level0 col1\" >V1</th> \n",
       "        <th class=\"col_heading level0 col2\" >V2</th> \n",
       "        <th class=\"col_heading level0 col3\" >Entity</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row0_col0\" class=\"data row0 col0\" >Preincubating</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row0_col1\" class=\"data row0 col1\" >817</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row0_col2\" class=\"data row0 col2\" >830</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row0_col3\" class=\"data row0 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row1_col0\" class=\"data row1 col0\" >mononuclear</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row1_col1\" class=\"data row1 col1\" >831</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row1_col2\" class=\"data row1 col2\" >842</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row1_col3\" class=\"data row1 col3\" >B-Cell</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row2_col0\" class=\"data row2 col0\" >cells</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row2_col1\" class=\"data row2 col1\" >843</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row2_col2\" class=\"data row2 col2\" >848</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row2_col3\" class=\"data row2 col3\" >I-Cell</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row3_col0\" class=\"data row3 col0\" >with</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row3_col1\" class=\"data row3 col1\" >849</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row3_col2\" class=\"data row3 col2\" >853</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row3_col3\" class=\"data row3 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row4_col0\" class=\"data row4 col0\" >anti</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row4_col1\" class=\"data row4 col1\" >854</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row4_col2\" class=\"data row4 col2\" >858</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row4_col3\" class=\"data row4 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row5_col0\" class=\"data row5 col0\" >-</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row5_col1\" class=\"data row5 col1\" >858</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row5_col2\" class=\"data row5 col2\" >859</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row5_col3\" class=\"data row5 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row6_col0\" class=\"data row6 col0\" >mycobacteria</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row6_col1\" class=\"data row6 col1\" >859</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row6_col2\" class=\"data row6 col2\" >871</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row6_col3\" class=\"data row6 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row7\" class=\"row_heading level0 row7\" >7</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row7_col0\" class=\"data row7 col0\" >antibodies</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row7_col1\" class=\"data row7 col1\" >872</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row7_col2\" class=\"data row7 col2\" >882</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row7_col3\" class=\"data row7 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row8\" class=\"row_heading level0 row8\" >8</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row8_col0\" class=\"data row8 col0\" >(</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row8_col1\" class=\"data row8 col1\" >883</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row8_col2\" class=\"data row8 col2\" >884</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row8_col3\" class=\"data row8 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row9\" class=\"row_heading level0 row9\" >9</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row9_col0\" class=\"data row9 col0\" >lepromatous</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row9_col1\" class=\"data row9 col1\" >884</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row9_col2\" class=\"data row9 col2\" >895</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row9_col3\" class=\"data row9 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row10\" class=\"row_heading level0 row10\" >10</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row10_col0\" class=\"data row10 col0\" >patients</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row10_col1\" class=\"data row10 col1\" >896</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row10_col2\" class=\"data row10 col2\" >904</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row10_col3\" class=\"data row10 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row11\" class=\"row_heading level0 row11\" >11</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row11_col0\" class=\"data row11 col0\" >'</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row11_col1\" class=\"data row11 col1\" >904</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row11_col2\" class=\"data row11 col2\" >905</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row11_col3\" class=\"data row11 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row12\" class=\"row_heading level0 row12\" >12</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row12_col0\" class=\"data row12 col0\" >sera</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row12_col1\" class=\"data row12 col1\" >906</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row12_col2\" class=\"data row12 col2\" >910</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row12_col3\" class=\"data row12 col3\" >B-Organism_substance</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row13\" class=\"row_heading level0 row13\" >13</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row13_col0\" class=\"data row13 col0\" >)</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row13_col1\" class=\"data row13 col1\" >910</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row13_col2\" class=\"data row13 col2\" >911</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row13_col3\" class=\"data row13 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row14\" class=\"row_heading level0 row14\" >14</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row14_col0\" class=\"data row14 col0\" >did</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row14_col1\" class=\"data row14 col1\" >912</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row14_col2\" class=\"data row14 col2\" >915</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row14_col3\" class=\"data row14 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row15\" class=\"row_heading level0 row15\" >15</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row15_col0\" class=\"data row15 col0\" >not</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row15_col1\" class=\"data row15 col1\" >916</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row15_col2\" class=\"data row15 col2\" >919</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row15_col3\" class=\"data row15 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row16\" class=\"row_heading level0 row16\" >16</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row16_col0\" class=\"data row16 col0\" >increase</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row16_col1\" class=\"data row16 col1\" >920</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row16_col2\" class=\"data row16 col2\" >928</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row16_col3\" class=\"data row16 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row17\" class=\"row_heading level0 row17\" >17</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row17_col0\" class=\"data row17 col0\" >the</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row17_col1\" class=\"data row17 col1\" >929</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row17_col2\" class=\"data row17 col2\" >932</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row17_col3\" class=\"data row17 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row18\" class=\"row_heading level0 row18\" >18</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row18_col0\" class=\"data row18 col0\" >CL</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row18_col1\" class=\"data row18 col1\" >933</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row18_col2\" class=\"data row18 col2\" >935</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row18_col3\" class=\"data row18 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row19\" class=\"row_heading level0 row19\" >19</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row19_col0\" class=\"data row19 col0\" >response</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row19_col1\" class=\"data row19 col1\" >936</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row19_col2\" class=\"data row19 col2\" >944</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row19_col3\" class=\"data row19 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row20\" class=\"row_heading level0 row20\" >20</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row20_col0\" class=\"data row20 col0\" >nor</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row20_col1\" class=\"data row20 col1\" >945</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row20_col2\" class=\"data row20 col2\" >948</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row20_col3\" class=\"data row20 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row21\" class=\"row_heading level0 row21\" >21</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row21_col0\" class=\"data row21 col0\" >the</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row21_col1\" class=\"data row21 col1\" >949</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row21_col2\" class=\"data row21 col2\" >952</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row21_col3\" class=\"data row21 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row22\" class=\"row_heading level0 row22\" >22</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row22_col0\" class=\"data row22 col0\" >phagocytosis</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row22_col1\" class=\"data row22 col1\" >953</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row22_col2\" class=\"data row22 col2\" >965</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row22_col3\" class=\"data row22 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row23\" class=\"row_heading level0 row23\" >23</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row23_col0\" class=\"data row23 col0\" >of</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row23_col1\" class=\"data row23 col1\" >966</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row23_col2\" class=\"data row23 col2\" >968</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row23_col3\" class=\"data row23 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row24\" class=\"row_heading level0 row24\" >24</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row24_col0\" class=\"data row24 col0\" >M</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row24_col1\" class=\"data row24 col1\" >969</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row24_col2\" class=\"data row24 col2\" >970</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row24_col3\" class=\"data row24 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row25\" class=\"row_heading level0 row25\" >25</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row25_col0\" class=\"data row25 col0\" >.</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row25_col1\" class=\"data row25 col1\" >970</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row25_col2\" class=\"data row25 col2\" >971</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row25_col3\" class=\"data row25 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row26\" class=\"row_heading level0 row26\" >26</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row26_col0\" class=\"data row26 col0\" >leprae</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row26_col1\" class=\"data row26 col1\" >972</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row26_col2\" class=\"data row26 col2\" >978</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row26_col3\" class=\"data row26 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row27\" class=\"row_heading level0 row27\" >27</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row27_col0\" class=\"data row27 col0\" >or</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row27_col1\" class=\"data row27 col1\" >979</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row27_col2\" class=\"data row27 col2\" >981</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row27_col3\" class=\"data row27 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row28\" class=\"row_heading level0 row28\" >28</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row28_col0\" class=\"data row28 col0\" >BCG</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row28_col1\" class=\"data row28 col1\" >982</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row28_col2\" class=\"data row28 col2\" >985</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row28_col3\" class=\"data row28 col3\" >O</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1level0_row29\" class=\"row_heading level0 row29\" >29</th> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row29_col0\" class=\"data row29 col0\" >.</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row29_col1\" class=\"data row29 col1\" >985</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row29_col2\" class=\"data row29 col2\" >986</td> \n",
       "        <td id=\"T_a2b30014_97fe_11e9_ba27_b05adad52aa1row29_col3\" class=\"data row29 col3\" >O</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display_html\n",
    "\n",
    "#Try to change this variable value\n",
    "sentence_to_visualize = 500\n",
    "\n",
    "with open(\"./AnEM/AnEM.train\", 'rb') as file_handle:\n",
    "    file_content = file_handle.read().decode('utf-8').strip()\n",
    "    annotated_sentences = file_content.split('\\n\\n')\n",
    "    sentence = annotated_sentences[sentence_to_visualize]\n",
    "    sentence = sentence.split()\n",
    "    sentence = [sentence[i:i + 4] for i in range(0, len(sentence), 4)]\n",
    "    cols=['Tokens','V1','V2','Entity']\n",
    "    df2 = pd.DataFrame(sentence, columns=cols)\n",
    "    df2_styler = df2.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Entities Distribution')\n",
    "    display_html(df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Example:\n",
    "![ola](Images/sentence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "d = np.array([['Train', 71697, 2815], ['Test', 45939, 1882], ['All', 117636, 4697]])\n",
    "cols=['Split', 'Tokens', 'Sentences']\n",
    "df = pd.DataFrame(d, columns=cols)\n",
    "\n",
    "d2 = np.array([['Anatomical_system', 51],['Cell', 776],['Cellular_component', 199],['Developing_anatomical_structure ', 39],\n",
    "              ['Immaterial_anatomical_entity', 60],['Multi-tissue_structure', 639],['Organ', 381],\n",
    "              ['Organism_subdivision', 162],['Organism_substance', 291],['Pathological_formation', 368],\n",
    "              ['Tissue', 169],['No Entity', 112000]])\n",
    "cols=['Token Entities:', 'Count']\n",
    "df2 = pd.DataFrame(d2, columns=cols)\n",
    "\n",
    "df1_styler = df.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Instances Distribution')\n",
    "df2_styler = df2.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Entities Distribution')\n",
    "\n",
    "display_html(df1_styler._repr_html_()+df2_styler._repr_html_(), raw=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:1 Prepare the dataset for the model\n",
    "### Read data from a ConLL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_conll(filename_end):\n",
    "    word_pos = 0\n",
    "    pos_pos= None\n",
    "    iob_pos = 3\n",
    "    sep = '\\t'\n",
    "    IOB= 'IOB2'\n",
    "    corpus_root=\"./AnEM\"\n",
    "\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(filename_end):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    try:\n",
    "                        file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    except:\n",
    "                        raise ValueError(\"Can't process!\")\n",
    "                    # Split sentences:\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        if annotated_sentence not in ['-DOCSTART- -X- O O', '-DOCSTART- -X- -X- O']:\n",
    "                            # Split words:\n",
    "                            annotated_tokens = [seq for seq in annotated_sentence.split('\\n')]\n",
    "                            standard_form_tokens = []\n",
    "\n",
    "                            for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                                if sep=='multispace':\n",
    "                                    annotations = annotated_token.split()   # Split annotations\n",
    "                                else:\n",
    "                                    annotations = annotated_token.split(sep)   # Split annotations\n",
    "                                try:\n",
    "                                    word, ner = annotations[word_pos], annotations[iob_pos]\n",
    "                                except:\n",
    "                                    print(annotations)\n",
    "                                    #raise ValueError(\"??\")\n",
    "                                if IOB == 'IOB2':\n",
    "                                    # This is for the Seminars_and_Job_postings\n",
    "                                    # data:\n",
    "                                    if ner=='0':\n",
    "                                        ner = 'O'\n",
    "                                    standard_form_tokens.append((word, ner))\n",
    "                                    conll_tokens = standard_form_tokens\n",
    "                                else:\n",
    "                                    raise ValueError('Variable IOB has wrong value.')\n",
    "\n",
    "                            yield [(w, iob) for w, iob in conll_tokens]\n",
    "                            \n",
    "                            \n",
    "data_train = list(read_conll('.train'))\n",
    "data_test = list(read_conll('.test'))\n",
    "\n",
    "#We can visualize the input for each sentence:\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sentence and label vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    sentences_array=[]\n",
    "    labels_array=[]\n",
    "    for data_input in data:\n",
    "        sentence=[]\n",
    "        labels=[]\n",
    "        for vec in data_input:\n",
    "            sentence.append(vec[0])\n",
    "            labels.append(vec[1])\n",
    "        sentences_array.append(sentence)\n",
    "        labels_array.append(labels)\n",
    "\n",
    "    return sentences_array,labels_array\n",
    "\n",
    "sentences_train, labels_train = transform(data_train)\n",
    "sentences_test, labels_test = transform(data_test)\n",
    "\n",
    "#We can visualize the input for each sentence:\n",
    "print(sentences_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Labels to Numeric Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]\n"
     ]
    }
   ],
   "source": [
    "def convert(labels_array):\n",
    "    for idx,label_vec in enumerate(labels_array):\n",
    "        for idx,label in enumerate(label_vec):\n",
    "            if label==\"B-Anatomical_system\":\n",
    "                label_vec[idx]=0\n",
    "            if label==\"I-Anatomical_system\":\n",
    "                label_vec[idx]=1\n",
    "            if label==\"B-Cell\":\n",
    "                label_vec[idx]=2\n",
    "            if label==\"I-Cell\":\n",
    "                label_vec[idx]=3\n",
    "            if label==\"B-Cellular_component\":\n",
    "                label_vec[idx]=4\n",
    "            if label==\"I-Cellular_component\":\n",
    "                label_vec[idx]=5\n",
    "            if label==\"B-Developing_anatomical_structure\":\n",
    "                label_vec[idx]=6\n",
    "            if label==\"I-Developing_anatomical_structure\":\n",
    "                label_vec[idx]=7\n",
    "            if label==\"B-Immaterial_anatomical_entity\":\n",
    "                label_vec[idx]=8\n",
    "            if label==\"I-Immaterial_anatomical_entity\":\n",
    "                label_vec[idx]=9\n",
    "            if label==\"B-Multi-tissue_structure\":\n",
    "                label_vec[idx]=10\n",
    "            if label==\"I-Multi-tissue_structure\":\n",
    "                label_vec[idx]=11\n",
    "            if label==\"B-Organ\":\n",
    "                label_vec[idx]=12\n",
    "            if label==\"I-Organ\":\n",
    "                label_vec[idx]=13\n",
    "            if label==\"B-Organism_subdivision\":\n",
    "                label_vec[idx]=14\n",
    "            if label==\"I-Organism_subdivision\":\n",
    "                label_vec[idx]=15\n",
    "            if label==\"B-Organism_substance\":\n",
    "                label_vec[idx]=16\n",
    "            if label==\"I-Organism_substance\":\n",
    "                label_vec[idx]=17\n",
    "            if label==\"B-Pathological_formation\":\n",
    "                label_vec[idx]=18\n",
    "            if label==\"I-Pathological_formation\":\n",
    "                label_vec[idx]=19\n",
    "            if label==\"B-Tissue\":\n",
    "                label_vec[idx]=20\n",
    "            if label==\"I-Tissue\":\n",
    "                label_vec[idx]=21\n",
    "            if label==\"O\":\n",
    "                label_vec[idx]=22\n",
    "    return labels_array\n",
    "\n",
    "\n",
    "labels_train = convert(labels_train)\n",
    "labels_test = convert(labels_test)\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional to convert to one-hot encoding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "def to_categorical(labels_array):\n",
    "    for idx,label_vec in enumerate(labels_array):\n",
    "        label_vec = keras.utils.to_categorical(label_vec, num_classes=23, dtype='float32')\n",
    "        labels_array[idx]=label_vec\n",
    "    return labels_array\n",
    "\n",
    "labels_train2 = to_categorical(labels_train)\n",
    "print(labels_train2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reports length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "tlen = [len(x) for x in sentences_train] \n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(tlen, bins=np.arange(max(tlen)), histtype='barstacked', linewidth=2)\n",
    "plt.title(\"Length of reports\")\n",
    "plt.ylabel('# of Instances', fontsize=12)\n",
    "plt.xlabel('Length of reports', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding to Input Shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2815, 50)\n",
      "[1364 3121  180   10  431 1939  377 2231  603   17 3122    1    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "(2815, 50, 23)\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  1]\n",
      " [ 0  0  0 ...  0  0  1]\n",
      " ...\n",
      " [22 22 22 ... 22 22 22]\n",
      " [22 22 22 ... 22 22 22]\n",
      " [22 22 22 ... 22 22 22]]\n",
      "(2815, 1150, 1)\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " ...\n",
      " [22]\n",
      " [22]\n",
      " [22]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "tokenizer.fit_on_texts(sentences_test)\n",
    "voc_size = len(tokenizer.word_index)   \n",
    "\n",
    "def convert2(x,y):\n",
    "    X_total = tokenizer.texts_to_sequences(x)\n",
    "    X_total = pad_sequences(X_total, maxlen=50, padding='post')\n",
    "    Y_total = pad_sequences(y, maxlen=50, padding='post', value=22)\n",
    "    return X_total,Y_total\n",
    "\n",
    "X_train, y_train = convert2(sentences_train, labels_train)\n",
    "X_test, y_test = convert2(sentences_test, labels_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "y_train = y_train.reshape(2815,-1,1)\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "y_test = y_test.reshape(1882,-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import array_ops\n",
    "def new_sparse_categorical_accuracy(y_true, y_pred):\n",
    "        y_pred_rank = ops.convert_to_tensor(y_pred).get_shape().ndims\n",
    "        y_true_rank = ops.convert_to_tensor(y_true).get_shape().ndims\n",
    "        # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "        if (y_true_rank is not None) and (y_pred_rank is not None) and (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
    "            y_true = array_ops.squeeze(y_true, [-1])\n",
    "        y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "        # If the predicted output and actual output types don't match, force cast them\n",
    "        # to match.\n",
    "        if K.dtype(y_pred) != K.dtype(y_true):\n",
    "            y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n",
    "        return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1:\n",
    "### Create Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,TimeDistributed, Input, Embedding,Bidirectional,LSTM,Dropout\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "def create_model(voc):\n",
    "    sequence_input = Input(shape=(50,), dtype='int32')\n",
    "    embedded_sequences = Embedding(voc, 32, input_length=50)(sequence_input)\n",
    "    bilstm = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=False))(embedded_sequences)\n",
    "    dense = Dense(256)(bilstm)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    preds = TimeDistributed(Dense(23, activation='softmax'))(drop)\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[new_sparse_categorical_accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_model(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "model.fit(x=X_train, y=y_train,validation_split=0.2,batch_size=32, epochs=1,verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True)])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "y_pred = y_pred.reshape(1882,-1,1)\n",
    "print(y_pred[0])\n",
    "print(y_test[0])\n",
    "#y_pred = y_pred.argmax(axis=-1)\n",
    "print(np.unique(y_test))\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 2:\n",
    "### Create Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ist181159\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras_contrib\\layers\\crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "C:\\Users\\ist181159\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras_contrib\\layers\\crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 64)            755904    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           197632    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50, 256)           65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 64)            16448     \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 50, 23)            2070      \n",
      "=================================================================\n",
      "Total params: 1,037,846\n",
      "Trainable params: 1,037,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,TimeDistributed, Input, Embedding,Bidirectional,LSTM,Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_model(voc,weights):\n",
    "    sequence_input = Input(shape=(50,), dtype='int32')\n",
    "    embedded_sequences = Embedding(voc,64, input_length=50)(sequence_input)\n",
    "    bilstm = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=False))(embedded_sequences)\n",
    "    dense = Dense(256)(bilstm)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    preds = TimeDistributed(Dense(64, activation='relu'))(drop)\n",
    "    crf = CRF(23)\n",
    "    out = crf(preds)\n",
    "    model = Model(inputs=sequence_input, outputs=out)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(voc_size,class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2815, 50, 23, 23, 23)\n",
      "[[[[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[1 0 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " [[[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[1 0 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]\n",
      "   [1 0 0 ... 0 0 0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]\n",
      "   [0 0 0 ... 0 0 1]]]]\n",
      "(2815, 608350, 1)\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9675c984c4294f37b9a100d0012f6900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=1, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb569cc7eda4ceaae1eab3318f2f164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=2252, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [77868672,1], In[1]: [23,23]\n\t [[Node: loss/crf_1_loss/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@training/RMSprop/gradients/loss/crf_1_loss/MatMul_1_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/crf_1_loss/Reshape_3, crf_1/chain_kernel/read)]]\n\t [[Node: metrics/crf_viterbi_accuracy/while_1/Switch_2/_191 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2362_metrics/crf_viterbi_accuracy/while_1/Switch_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopmetrics/crf_viterbi_accuracy/while_1/ExpandDims/dim/_73)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7d8ade89a929>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleave_inner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[1;32m-> 1454\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [77868672,1], In[1]: [23,23]\n\t [[Node: loss/crf_1_loss/MatMul_1 = MatMul[T=DT_FLOAT, _class=[\"loc:@training/RMSprop/gradients/loss/crf_1_loss/MatMul_1_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](loss/crf_1_loss/Reshape_3, crf_1/chain_kernel/read)]]\n\t [[Node: metrics/crf_viterbi_accuracy/while_1/Switch_2/_191 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2362_metrics/crf_viterbi_accuracy/while_1/Switch_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopmetrics/crf_viterbi_accuracy/while_1/ExpandDims/dim/_73)]]"
     ]
    }
   ],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "labels_train2 = to_categorical(labels_train)\n",
    "print(labels_train2)\n",
    "y_train = pad_sequences(labels_train2, maxlen=50, padding='post', value=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1])\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "y_train = y_train.reshape(2815,-1,1)\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "model.fit(x=X_train, y=y_train,validation_split=0.2,batch_size=128, epochs=1,verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True)])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,TimeDistributed, Input, Embedding,Bidirectional,LSTM,Dropout\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "A weighted version of categorical_crossentropy for keras (2.0.6). This lets you apply a weight to unbalanced classes.\n",
    "@url: https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
    "@author: wassname\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "def create_model(voc,weights):\n",
    "    sequence_input = Input(shape=(50,), dtype='int32')\n",
    "    embedded_sequences = Embedding(voc, 64, input_length=50)(sequence_input)\n",
    "    bilstm = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state= False))(embedded_sequences)\n",
    "    dense = Dense(256)(bilstm)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    preds = TimeDistributed(Dense(23, activation='softmax'))(drop)\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "    # Compile model\n",
    "    model.compile(loss=weighted_categorical_crossentropy(weights), optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "train = y_train.argmax(axis=-1)\n",
    "unique, counts = np.unique(train, return_counts=True)\n",
    "print(unique)\n",
    "labels_dict = dict(zip(unique, counts))\n",
    "total = sum(labels_dict.values())\n",
    "labels_dict.update((x, total/y) for x, y in labels_dict.items())\n",
    "class_weight = np.array(list(labels_dict.values()))\n",
    "print(class_weight)\n",
    "                        \n",
    "model=create_model(voc_size,class_weight)\n",
    "model.fit(x=X_train, y=y_train,validation_split=0.2,batch_size=64, epochs=10,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,TimeDistributed, Input, Embedding,Bidirectional,LSTM,Dropout\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def naive_over_sampling(x_train,y_train):\n",
    "    x_train_final = x_train[:]\n",
    "    y_train_final = y_train[:]\n",
    "    for idx, sentence in enumerate(y_train):\n",
    "        for output in sentence:\n",
    "            if(np.argmax(output)!=22):\n",
    "                x_train_final = np.append(x_train_final,x_train[idx]])\n",
    "                y_train_final = np.append(y_train_final,sentence])\n",
    "                break\n",
    "    return x_train_final,y_train_final\n",
    "\n",
    "def create_model(voc):\n",
    "    sequence_input = Input(shape=(50,), dtype='int32')\n",
    "    embedded_sequences = Embedding(voc, 64, input_length=50)(sequence_input)\n",
    "    bilstm = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state= False))(embedded_sequences)\n",
    "    dense = Dense(256)(bilstm)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    preds = TimeDistributed(Dense(23, activation='softmax'))(drop)\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "X_train,y_train = naive_over_sampling(X_train,y_train)\n",
    "print(metrics.flat_classification_report(y_train, y_train, digits=3))\n",
    "#model=create_model(voc_size)\n",
    "#model.fit(x=X_train, y=y_train,validation_split=0.2,batch_size=64, epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "test = y_test.argmax(axis=-1)\n",
    "#sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
    "print(metrics.flat_classification_report(test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
