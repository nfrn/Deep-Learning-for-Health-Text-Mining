{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bio-Entity Recognition Task at BioNLP/NLPBA 2004:\n",
    "### Task Definition:\n",
    "The task aims to identify and classify technical terms in the domain of molecular biology that correspond to instances of concepts that are of interest to biologists.\n",
    "### Data:\n",
    "The training data used in the task came from the GENIA version 3.02 corpus, This was formed from a controlled search on MEDLINE using the MeSH terms 'human', 'blood cells' and 'transcription factors'. From this search, 2,000 abstracts were selected and hand annotated according to a small taxonomy of 48 classes based on a chemical classification. Among the classes, 36 terminal classes were used to annotate the GENIA corpus. For the shared task we decided however to simplify the 36 classes and used only the classes protein, DNA, RNA, cell line and cell type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Visualization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display_html\n",
    "\n",
    "#Try to change this variable value\n",
    "sentence_to_visualize = 0\n",
    "\n",
    "with open(\"./BetterDataset/data.train\", 'rb') as file_handle:\n",
    "    file_content = file_handle.read().decode('utf-8').strip()\n",
    "    annotated_sentences = file_content.split('\\r\\n\\r\\n')\n",
    "    sentence = annotated_sentences[sentence_to_visualize]\n",
    "    sentence = sentence.split()\n",
    "    sentence = [sentence[i:i + 2] for i in range(0, len(sentence), 2)]\n",
    "    cols=['Tokens','Entity']\n",
    "    df2 = pd.DataFrame(sentence, columns=cols)\n",
    "    df2_styler = df2.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Entities Distribution')\n",
    "    display_html(df2_styler._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task:1 Prepare the dataset for the model\n",
    "### Read data from a ConLL file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def read_conll(filename_end):\n",
    "    word_pos = 0\n",
    "    pos_pos= None\n",
    "    iob_pos = 3\n",
    "    sep = '\\t'\n",
    "    IOB= 'IOB2'\n",
    "    corpus_root=\"./BetterDataset\"\n",
    "\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(filename_end):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    try:\n",
    "                        file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    except:\n",
    "                        raise ValueError(\"Can't process!\")\n",
    "                    \n",
    "                    annotated_sentences = file_content.split('\\r\\n\\r\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\r\\n')]\n",
    "                        all_tokens = []\n",
    "                        #print(annotated_tokens)\n",
    "                        for annotation in annotated_tokens:\n",
    "                            conll_tokens = annotation.split(sep)\n",
    "                            #print(conll_tokens)\n",
    "                            all_tokens.append(conll_tokens)\n",
    "                        yield all_tokens\n",
    "                            \n",
    "                            \n",
    "data_train = list(read_conll('.train'))\n",
    "data_test = list(read_conll('.test'))\n",
    "\n",
    "#We can visualize the input for each sentence:\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def visualizeClassImbalance(data):\n",
    "    # View the class count distribution\n",
    "    array = np.array(data)\n",
    "    \n",
    "    dic={\"O\":0,\"B-protein\":0,\"I-protein\":0,\"B-DNA\":0, \"I-DNA\":0, \"B-RNA\":0, \"I-RNA\":0,\n",
    "         \"B-cell_type\":0,\"I-cell_type\":0, \"B-cell_line\":0, \"I-cell_line\":0}\n",
    "    for sub_array in array:\n",
    "        for sub_sub_array in sub_array:\n",
    "            dic[sub_sub_array[1]]=dic.get(sub_sub_array[1])+1\n",
    "    f, ax = plt.subplots(figsize=(18,5)) \n",
    "    plt.bar(dic.keys(), dic.values(),width=0.5, color='C0')\n",
    "    plt.title(\"Number of instances per label\")\n",
    "    plt.ylabel('# of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Label', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    dic={\"B-protein\":0,\"I-protein\":0,\"B-DNA\":0, \"I-DNA\":0, \"B-RNA\":0, \"I-RNA\":0,\n",
    "         \"B-cell_type\":0,\"I-cell_type\":0, \"B-cell_line\":0, \"I-cell_line\":0}\n",
    "    for sub_array in array:\n",
    "        for sub_sub_array in sub_array:\n",
    "            if sub_sub_array[1] in dic:\n",
    "                dic[sub_sub_array[1]]=dic.get(sub_sub_array[1])+1\n",
    "    f, ax = plt.subplots(figsize=(18,5)) \n",
    "    plt.bar(dic.keys(), dic.values(),width=0.5, color='C0')\n",
    "    plt.title(\"Number of instances per label\")\n",
    "    plt.ylabel('# of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Label', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    dic={\"protein\":0,\"DNA\":0,\"RNA\":0,\"cell_type\":0,\"cell_line\":0}\n",
    "    for sub_array in array:\n",
    "        for sub_sub_array in sub_array:\n",
    "            if sub_sub_array[1][2:] in dic:\n",
    "                dic[sub_sub_array[1][2:]]=dic.get(sub_sub_array[1][2:])+1\n",
    "    f, ax = plt.subplots(figsize=(18,5)) \n",
    "    plt.bar(dic.keys(), dic.values(),width=0.5, color='C0')\n",
    "    plt.title(\"Number of instances per label\")\n",
    "    plt.ylabel('# of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Label', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "data_train = list(read_conll('.train'))\n",
    "data_test = list(read_conll('.test'))\n",
    "\n",
    "visualizeClassImbalance(data_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sentence and label vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    sentences_array=[]\n",
    "    labels_array=[]\n",
    "    for data_input in data:\n",
    "        sentence=[]\n",
    "        labels=[]\n",
    "        for vec in data_input:\n",
    "            sentence.append(vec[0])\n",
    "            labels.append(vec[1])\n",
    "        sentences_array.append(sentence)\n",
    "        labels_array.append(labels)\n",
    "\n",
    "    return sentences_array,labels_array\n",
    "\n",
    "sentences_train, labels_train = transform(data_train)\n",
    "sentences_test, labels_test = transform(data_test)\n",
    "\n",
    "#We can visualize the input for each sentence:\n",
    "print(sentences_train[0])\n",
    "print(labels_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Labels to Numeric Values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def convert(labels_array):\n",
    "    labels_array_changed = np.copy(labels_array)\n",
    "    for idx,label_vec in enumerate(labels_array_changed):\n",
    "        for idx,label in enumerate(label_vec):\n",
    "            if label==\"O\":\n",
    "                label_vec[idx]=0\n",
    "            if label==\"B-protein\":\n",
    "                label_vec[idx]=1\n",
    "            if label==\"I-protein\":\n",
    "                label_vec[idx]=2\n",
    "            if label==\"B-DNA\":\n",
    "                label_vec[idx]=3\n",
    "            if label==\"I-DNA\":\n",
    "                label_vec[idx]=4\n",
    "            if label==\"B-RNA\":\n",
    "                label_vec[idx]=5\n",
    "            if label==\"I-RNA\":\n",
    "                label_vec[idx]=6\n",
    "            if label==\"B-cell_type\":\n",
    "                label_vec[idx]=7\n",
    "            if label==\"I-cell_type\":\n",
    "                label_vec[idx]=8\n",
    "            if label==\"B-cell_line\":\n",
    "                label_vec[idx]=9\n",
    "            if label==\"I-cell_line\":\n",
    "                label_vec[idx]=10\n",
    "    return labels_array_changed\n",
    "\n",
    "labels_train_transformed = convert(labels_train)\n",
    "labels_test_transformed = convert(labels_test)\n",
    "print(labels_train_transformed.shape)\n",
    "print(labels_test_transformed.shape)\n",
    "print(labels_train_transformed[0])\n",
    "print(labels_test_transformed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional to convert to one-hot encoding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "def to_categorical(labels_array):\n",
    "    print(labels_array.shape)\n",
    "    labels_train_changed = np.copy(labels_array)\n",
    "    for idx,label_vec in enumerate(labels_train_changed):\n",
    "        label_vec = keras.utils.to_categorical(label_vec, num_classes=11, dtype='float32')\n",
    "        labels_train_changed[idx]=label_vec\n",
    "    return labels_train_changed\n",
    "\n",
    "print(labels_train_transformed[0])\n",
    "example = to_categorical(labels_train_transformed)\n",
    "print(example[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reports length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "tlen = [len(x) for x in sentences_train] \n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(tlen, bins=np.arange(max(tlen)), histtype='barstacked', linewidth=2)\n",
    "plt.title(\"Length of reports\")\n",
    "plt.ylabel('# of Instances', fontsize=12)\n",
    "plt.xlabel('Length of reports', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding to Input Shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "tokenizer.fit_on_texts(sentences_test)\n",
    "voc_size = len(tokenizer.word_index)+1\n",
    "\n",
    "def convert2(x,y):\n",
    "    X_total = tokenizer.texts_to_sequences(x)\n",
    "    X_total = pad_sequences(X_total, maxlen=50, padding='post')\n",
    "    Y_total = pad_sequences(y, maxlen=50, padding='post', value=0)\n",
    "    return X_total,Y_total\n",
    "\n",
    "X_train, y_train = convert2(sentences_train, labels_train_transformed)\n",
    "X_test, y_test = convert2(sentences_test, labels_test_transformed)\n",
    "\n",
    "print(\"Input Shapes:\")\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"Target Shapes:\")\n",
    "y_train = np.expand_dims(y_train, axis=2)\n",
    "print(y_train.shape)\n",
    "y_test = np.expand_dims(y_test, axis=2)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import array_ops\n",
    "def new_sparse_categorical_accuracy(y_true, y_pred):\n",
    "        y_pred_rank = ops.convert_to_tensor(y_pred).get_shape().ndims\n",
    "        y_true_rank = ops.convert_to_tensor(y_true).get_shape().ndims\n",
    "        # If the shape of y_true is (num_samples, 1), squeeze to (num_samples,)\n",
    "        if (y_true_rank is not None) and (y_pred_rank is not None) and (len(K.int_shape(y_true)) == len(K.int_shape(y_pred))):\n",
    "            y_true = array_ops.squeeze(y_true, [-1])\n",
    "        y_pred = math_ops.argmax(y_pred, axis=-1)\n",
    "        # If the predicted output and actual output types don't match, force cast them\n",
    "        # to match.\n",
    "        if K.dtype(y_pred) != K.dtype(y_true):\n",
    "            y_pred = math_ops.cast(y_pred, K.dtype(y_true))\n",
    "        return math_ops.cast(math_ops.equal(y_true, y_pred), K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt 1:\n",
    "### Create Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,TimeDistributed, Input, Embedding,Bidirectional,LSTM,Dropout\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "def create_model(voc):\n",
    "    sequence_input = Input(shape=(50,), dtype='int32')\n",
    "    embedded_sequences = Embedding(voc, 32, input_length=50)(sequence_input)\n",
    "    bilstm = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=False))(embedded_sequences)\n",
    "    dense = Dense(256)(bilstm)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    preds = TimeDistributed(Dense(11, activation='softmax'))(drop)\n",
    "    model = Model(inputs=sequence_input, outputs=preds)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[new_sparse_categorical_accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = create_model(voc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "model.fit(x=X_train, y=y_train,validation_split=0.2,batch_size=32, epochs=3,verbose=0, callbacks=[TQDMNotebookCallback(leave_inner=True)])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "y_pred = np.expand_dims(y_pred, axis=2)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "def checkifAllOut(sentence):\n",
    "    for output in sentence:\n",
    "        if(output!=0):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def RemoveOnlyOtherSentences(x_train,y_train):\n",
    "    x_train_final = []\n",
    "    y_train_final = []\n",
    "    for idx, sentence in enumerate(y_train):\n",
    "        if(checkifAllOut(sentence)):\n",
    "            x_train_final.append(x_train[idx])\n",
    "            y_train_final.append(sentence)\n",
    "    return np.array(x_train_final),np.array(y_train_final)\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)\n",
    "\n",
    "X_train_new, y_train_new = RemoveOnlyOtherSentences(X_train,y_train)\n",
    "\n",
    "unique, counts = np.unique(y_train_new, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)\n",
    "\n",
    "print(X_train_new.shape)\n",
    "print(y_train_new.shape)\n",
    "\n",
    "model.fit(x=X_train_new, y=y_train_new,validation_split=0.2,batch_size=64,\n",
    "          epochs=3,verbose=0,\n",
    "          callbacks=[TQDMNotebookCallback(leave_inner=True)])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "y_pred = np.expand_dims(y_pred, axis=2)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,TimeDistributed, Input, Embedding,Bidirectional,LSTM,Dropout\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib import losses, metrics\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_model(voc):\n",
    "    sequence_input = Input(shape=(50,), dtype='int32')\n",
    "    embedded_sequences = Embedding(voc,64, input_length=50)(sequence_input)\n",
    "    bilstm = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True, return_state=False))(embedded_sequences)\n",
    "    dense = Dense(256)(bilstm)\n",
    "    drop = Dropout(0.2)(dense)\n",
    "    preds = TimeDistributed(Dense(64, activation='relu'))(drop)\n",
    "    crf = CRF(11)\n",
    "    out = crf(preds)\n",
    "    model = Model(inputs=sequence_input, outputs=out)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=losses.crf_loss, metrics=[metrics.crf_accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = create_model(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_input = np.squeeze(y_train_new, axis=2)\n",
    "from keras.utils import to_categorical\n",
    "from sklearn_crfsuite import metrics\n",
    "# One-Hot encode\n",
    "y = [to_categorical(i, num_classes=11) for i in y_train_new]  # n_tags+1(PAD)\n",
    "y = np.array(y)\n",
    "print(y.shape)\n",
    "\n",
    "model.fit(x=X_train_new, y=y,validation_split=0.2,batch_size=64,\n",
    "          epochs=3,verbose=0,\n",
    "          callbacks=[TQDMNotebookCallback(leave_inner=True)])\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "y_pred = np.expand_dims(y_pred, axis=2)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_lab_env",
   "language": "python",
   "name": "my_lab_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
