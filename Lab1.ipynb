{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "my_lab_env",
      "language": "python",
      "name": "my_lab_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Lab1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nfrn/Deep-Learning-for-Health-Text-Mining/blob/master/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5XHDezrYPqp"
      },
      "source": [
        "# Clinical Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_SdhGeiYRDA"
      },
      "source": [
        "! pip install requests\n",
        "! pip install pandas\n",
        "! pip install nltk\n",
        "! pip install matplotlib\n",
        "! pip install Keras\n",
        "! pip install tensorflow\n",
        "! pip install scikit-learn\n",
        "! pip install keras-tqdm\n",
        "! pip install sklearn-crfsuite\n",
        "! pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxR5_P6UZ7L9"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqfJZzMDbdH6"
      },
      "source": [
        "pip install --upgrade tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bo7afh3YPqs"
      },
      "source": [
        "## Table of Contents:\n",
        "* Description of the Data\n",
        "* Creation of the Dataset\n",
        "* Naive Bayes\n",
        "* Support Vector Machines\n",
        "* Multi-Layer-Perceptron\n",
        "* Convolutional Neural Networks\n",
        "* Recurrent Neural Networks\n",
        "* Contest. Who can build the best Model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWDpkjN2YPqu"
      },
      "source": [
        "## Dataset Open-i:\n",
        "### Description:\n",
        "* Open-i service of the National Library of Medicine enables search and retrieval of abstracts and images (including charts, graphs, clinical images, etc.) from the open source literature, and biomedical image collections. Searching may be done using text queries as well as query images.\n",
        "* Open-i provides access to over 3.7 million images from about 1.2 million PubMed CentralÂ® articles; 7,470 chest x-rays with 3,955 radiology reports; 67,517 images from NLM History of Medicine collection; and 2,064 orthopedic illustrations.\n",
        "* Link to the website: https://openi.nlm.nih.gov/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUvu9-ZVYPqw"
      },
      "source": [
        "### Data Visualization:\n",
        "# ![ola](https://github.com/nfrn/Deep-Learning-for-Health-Text-Mining/blob/master/Images/example.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRfMy_XjYPqy"
      },
      "source": [
        "### Download Reports XML:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tl1ak3NYPqz"
      },
      "source": [
        "import requests\n",
        "url = \"https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\"\n",
        "filename = url.split(\"/\")[-1]\n",
        "with open(filename, \"wb\") as f:\n",
        "    r = requests.get(url)\n",
        "    f.write(r.content)\n",
        "print(\"Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvadCxGqYPq7"
      },
      "source": [
        "#### After the script finishes, a new file NLMCXR_reports.tgz should be visible on your working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdsc2QX5YPq9"
      },
      "source": [
        "### Extract files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx6ZXazTYPq_"
      },
      "source": [
        "import tarfile\n",
        "fname=\"NLMCXR_reports.tgz\"\n",
        "tar = tarfile.open(fname)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "print(\"Finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJsECjkjYPrH"
      },
      "source": [
        "#### After the script finishes, a new folder is created: \"./ecgen-radiology\" with all the .xml files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBgs-q7YYPrJ"
      },
      "source": [
        "### Visualize XML files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caw9976KYPrO"
      },
      "source": [
        "from pygments import highlight\n",
        "from pygments.lexers import XmlLexer\n",
        "from xml.dom import minidom\n",
        "\n",
        "from pygments.formatters import HtmlFormatter\n",
        "import IPython\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "def display_xml_nice(xml_element):\n",
        "    formatter = HtmlFormatter()\n",
        "    xml_indented = xml_element.toprettyxml(indent='  ',newl=\"\")\n",
        "    IPython.display.display(HTML('<style type=\"text/css\">{}</style>    {}'.format(\n",
        "    formatter.get_style_defs('.highlight'),\n",
        "    highlight(xml_indented, XmlLexer(), formatter))))\n",
        "    \n",
        "domf = minidom.parse('./ecgen-radiology/1.xml')\n",
        "display_xml_nice(domf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVXG94niYPrW"
      },
      "source": [
        "### Important XML Tags\n",
        "* AbstractText Label=\"COMPARISON\"\n",
        "* AbstractText Label=\"INDICATION\n",
        "* AbstractText Label=\"FINDINGS\"\n",
        "* AbstractText Label=\"IMPRESSION\"\n",
        "* MeSH: Medical Subject Headings is the NLM controlled vocabulary thesaurus used for indexing articles for PubMed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1J1N5lOYPrY"
      },
      "source": [
        "### Create Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rQT5uwmYPrZ"
      },
      "source": [
        "import glob\n",
        "import re\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xml.dom.minidom as minidom\n",
        "\n",
        "def getText(file):\n",
        "    # Extract the clinical report by mergin the Text Labels\n",
        "    stringa = file.find(\"<Abstract>\")\n",
        "    stringb = file.find(\"</Abstract>\")\n",
        "    if stringa == -1 or stringb == -1:\n",
        "        print(\"No abstract\")\n",
        "        return \"NO ABSTRACT\"\n",
        "    all = file[stringa:stringb]\n",
        "    all = re.sub(\"<Abstract>\", \"\", all)\n",
        "    all = re.sub(\"<AbstractText Label=\", \"\", all)\n",
        "    all = re.sub(\"</AbstractText>\", \"\", all)\n",
        "    all = re.sub(\" +\", \" \", all)\n",
        "    all = re.sub('\\\"', \"\", all)\n",
        "    all = re.sub('>', \" \", all)\n",
        "    all = re.sub('\\n', \"\", all)\n",
        "    return all\n",
        "\n",
        "def getLabels(doc):\n",
        "    # Extract the MeSH labels from the xml doument\n",
        "    value = []\n",
        "    for idx2, node in enumerate(doc.getElementsByTagName('MeSH')):\n",
        "        for elem in node.childNodes:\n",
        "            string = elem.toxml()\n",
        "            string = re.sub(\"<automatic>\", \"\", string)\n",
        "            string = re.sub(\"</automatic>\", \"\", string)\n",
        "            string = re.sub(\"<major>\", \"\", string)\n",
        "            string = re.sub(\"</major>\", \"\", string)\n",
        "            string = re.sub(\"\\n\", \"\", string)\n",
        "            if \"  \" not in string:\n",
        "                value.append(string)\n",
        "\n",
        "    return value\n",
        "\n",
        "def xmlToDF():\n",
        "    # Go through each XML file and saving the clinical report and the Mesh Labels in a csv dataset\n",
        "    df = pd.DataFrame(columns=[\"Labels\",\"Report\"])\n",
        "    for idx, file in enumerate(glob.glob(\"./ecgen-radiology/*.xml\")):\n",
        "        doc = minidom.parse(file)\n",
        "        file = doc.toxml()\n",
        "\n",
        "        labels = getLabels(doc)\n",
        "        text = getText(file)\n",
        "\n",
        "        df.at[idx, \"Labels\"]= labels\n",
        "        df.at[idx, \"Report\"]= text\n",
        "\n",
        "\n",
        "    df.to_csv(\"dataset.csv\",index=False)\n",
        "    print(\"Finished\")\n",
        "    \n",
        "xmlToDF()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0bfVlwHYPrf"
      },
      "source": [
        "### Visualize Draft Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM-lYZPnYPrh"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display_html\n",
        "df = pd.read_csv(\"dataset.csv\", nrows=100)\n",
        "df2_styler = df.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Dataset Entries')\n",
        "display_html(df2_styler._repr_html_(), raw=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v_9MuDmYPr2"
      },
      "source": [
        "### Polish Draft Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "objOaZy6YPr4"
      },
      "source": [
        "from IPython.display import display_html\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def labelsprocessing():\n",
        "    # Clean the pontuations of the Labels we collected\n",
        "    df = pd.read_csv(\"dataset.csv\")\n",
        "    for idx,x in enumerate(df[\"Labels\"]):\n",
        "        x = re.sub(\"\",\"\",x)\n",
        "        x = re.sub(\"\\[\", \"\", x)\n",
        "        x = re.sub(\"]\", \"\", x)\n",
        "        x = re.sub(\"/\", \" \", x)\n",
        "        x = re.sub(\",\", \" \", x)\n",
        "        x = re.sub(\"'\", \" \", x)\n",
        "        x = re.sub(\"{ }+ \", \" \", x)\n",
        "        x = re.sub(' +', ' ',x)\n",
        "        words = word_tokenize(x)\n",
        "        words = [word.lower() for word in words]\n",
        "        df.at[idx, \"Labels\"]= \" \".join(words)\n",
        "        \n",
        "    # Visualize the dataset\n",
        "    df.to_csv(\"dataset.csv\",index=False)\n",
        "    df_styler = df.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Dataset Entries')\n",
        "    display_html(df_styler._repr_html_(), raw=True)\n",
        "        \n",
        "labelsprocessing()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlQrBOvFYPsC"
      },
      "source": [
        "from IPython.display import display_html\n",
        "\n",
        "ITEMS = {'No findings':['normal'],\n",
        "             'Enlarged Cardiomediastinum': ['enlarged mediastinum'],\n",
        "             'Cardiomegaly': ['cardiomegaly'],\n",
        "             'Airspace Opacity': ['opacity'],\n",
        "             'Lung Lesion': ['lung'],\n",
        "             'Edema': ['edema','edemas'],\n",
        "             'Consolidation': ['consolidation'],\n",
        "             'Pneumonia': ['pneumonia'],\n",
        "             'Atelectasis': ['atelectasis'],\n",
        "             'Pneumothorax': ['pneumothorax','hydropneumothorax'],\n",
        "             'Pleural Effusion': ['pleural effusion','pleural effusions'],\n",
        "             'Pleural Other': [\"pleural thickening\",'pleural diseases'],\n",
        "             'Fracture': ['fracture','fractures'],\n",
        "             'Support Devices': ['medical device']}\n",
        "\n",
        "def transformToLabels():\n",
        "    \n",
        "    df = pd.read_csv(\"dataset.csv\")\n",
        "    \n",
        "    #Initialize each label column with 0\n",
        "    for label in ITEMS.keys():\n",
        "        df[label] = 0\n",
        "        \n",
        "    #For each report, we check if the Mesh contains the 14 clinical labels we plan to identify.\n",
        "    for label in ITEMS.keys():\n",
        "        for code in ITEMS.get(label):\n",
        "            if code == \"normal\":\n",
        "                idx = df.index[df['Labels'] == \"normal\"]\n",
        "                df.loc[idx,label] = 1\n",
        "            else:\n",
        "                df.loc[df['Labels'].str.contains(code), label] = 1\n",
        "        \n",
        "    # Visualize the dataset\n",
        "    df.to_csv(\"dataset.csv\",index=False)\n",
        "    df_styler = df.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Dataset Entries')\n",
        "    display_html(df_styler._repr_html_(), raw=True)\n",
        "    \n",
        "\n",
        "transformToLabels()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fDF3obEYPsK"
      },
      "source": [
        "# Instances with no \"patologies labels\" with 1s should have the \"label No Findings\" with 1.\n",
        "from IPython.display import display_html\n",
        "import pandas as pd\n",
        "\n",
        "LABELS = ['No findings','Enlarged Cardiomediastinum','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "\n",
        "def noFindings():\n",
        "    # If a report as all labels with 0s, we give 1 to the No Findings Labels\n",
        "    df2 = pd.read_csv(\"dataset.csv\")\n",
        "    df = pd.read_csv(\"dataset.csv\",usecols=LABELS)\n",
        "    values = df.sum(axis=1)\n",
        "    for idx,total in enumerate(values):\n",
        "        if total==0:\n",
        "            df2.at[idx,'No findings']=1\n",
        "            \n",
        "    # Visualize the dataset\n",
        "    df2 = df2.drop(['Labels'], axis=1)\n",
        "    df2.to_csv(\"dataset.csv\",index=False)\n",
        "    df_styler = df2.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Dataset Entries')\n",
        "    display_html(df_styler._repr_html_(), raw=True)\n",
        "    \n",
        "noFindings()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMvNkQ51YPsS"
      },
      "source": [
        "### Visualize Final Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRQGSzgaYPsU"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display_html\n",
        "df = pd.read_csv(\"dataset.csv\", nrows=100)\n",
        "df2_styler = df.reset_index(drop=True).style.set_table_attributes(\"style='display:inline'\").set_caption('Dataset Entries')\n",
        "display_html(df2_styler._repr_html_(), raw=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wc8-5o1YPsb"
      },
      "source": [
        "### Visualize Dataset Statistics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfWG2Y2dYPse"
      },
      "source": [
        "#### Visualize the Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHm_r7apYPsh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "LABELS = ['No findings','Enlarged Cardiomediastinum','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "def visualizeClassImbalance():\n",
        "    # View the class count distribution\n",
        "    df = pd.read_csv(\"dataset.csv\",usecols =LABELS)\n",
        "    counts = []\n",
        "    categories = list(df.columns.values)\n",
        "    for i in categories:\n",
        "        counts.append((i, df[i].sum()))\n",
        "    df_stats = pd.DataFrame(counts)\n",
        "    \n",
        "    df_stats.plot(kind='bar', legend=False, grid=True, figsize=(8, 5))\n",
        "    plt.title(\"Number of instances per label\")\n",
        "    plt.ylabel('# of Occurrences', fontsize=12)\n",
        "    plt.xlabel('Label', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "visualizeClassImbalance()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRfEVjncYPsp"
      },
      "source": [
        "#### Visualize Reports Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFzR-m65YPss"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "def visualizeReportsLength():\n",
        "    df = pd.read_csv(\"dataset.csv\",usecols =['Report'])\n",
        "    df['text_length'] = df['Report'].str.split().str.len()\n",
        "    tlen = df['text_length'].values\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.hist(tlen, bins=np.arange(max(tlen)), histtype='barstacked', linewidth=2)\n",
        "    plt.title(\"Length of reports\")\n",
        "    plt.ylabel('# of Instances', fontsize=12)\n",
        "    plt.xlabel('Length of reports', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "visualizeReportsLength()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk3zzHp6YPs1"
      },
      "source": [
        "### Preprocess Dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz5aiSXIYPs2"
      },
      "source": [
        "#### Extract the clinical text reports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnzuILn4YPs3"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "\n",
        "def prepareTextFeatures():\n",
        "    df = pd.read_csv(\"dataset.csv\",usecols =['Report'])\n",
        "    texts = df.values\n",
        "    processeddocs = []\n",
        "    for idx, sentence in enumerate(texts):\n",
        "        processedSentence = text_to_word_sequence(sentence[0])\n",
        "        corpus = ''\n",
        "        for word in processedSentence:\n",
        "            corpus = corpus + ' ' + word\n",
        "        processeddocs.append(corpus)\n",
        "        \n",
        "    print(processeddocs[0])\n",
        "    return processeddocs\n",
        "corpus = prepareTextFeatures()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV-F4JEsYPs-"
      },
      "source": [
        "### TF-IDF = short for term frequencyâinverse document frequency:\n",
        "* Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length: $TF(t,d) = \\frac{Count(t)}{Length(d)}$.\n",
        "* IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
        "$IDF(t,d) = \\ln{\\frac{Count(d)}{Count(d, t \\in d)}}$.\n",
        "* TF-IDF: Combines both approaches, by computing the following: ${TF-IDF}(t,d) = TF(t,d) \\cdot IDF(t,d)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLie-5WeYPs_"
      },
      "source": [
        "### Auc-ROC Metric\n",
        "* Performance measurement important for multi-class classification\n",
        "* It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s.\n",
        "* By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n",
        "* You can understand more about this metric in the following article: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnUkjIz7YPtB"
      },
      "source": [
        "### Attempt 1 Naive Bayes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seyJWQriYPtG"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction import text\n",
        "import pandas as pd\n",
        "\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "\n",
        "ngram=4\n",
        "minf=0.05\n",
        "maxf=0.95\n",
        "stopwords = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(sublinear_tf=True,norm='l2',stop_words=stopwords,\n",
        "                                          strip_accents='ascii', lowercase=True, ngram_range=(1, ngram),\n",
        "                                          min_df=minf, max_df=maxf)),\n",
        "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "                    fit_prior=True, class_prior=None))),\n",
        "            ])\n",
        "\n",
        "total=0\n",
        "for category in LABELS:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = NB_pipeline.predict(X_test)\n",
        "    fpr, tpr, _ = roc_curve(test[category], prediction)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPXXDfWmYPtN"
      },
      "source": [
        "### Attempt 2 Support Vector Machines (Linear Kernel):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST46t6L5YPtO"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction import text\n",
        "import pandas as pd\n",
        "\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "\n",
        "ngram=4\n",
        "minf=0.05\n",
        "maxf=0.95\n",
        "stopwords = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(sublinear_tf=True,norm='l2',stop_words=stopwords,\n",
        "                                          strip_accents='ascii', lowercase=True, ngram_range=(1, ngram),\n",
        "                                          min_df=minf, max_df=maxf)),\n",
        "                ('clf', OneVsRestClassifier(svm.SVC(kernel='linear',gamma='scale')))])\n",
        "\n",
        "total=0\n",
        "for category in LABELS:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = NB_pipeline.predict(X_test)\n",
        "    fpr, tpr, _ = roc_curve(test[category], prediction)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53f2UD-dYPtb"
      },
      "source": [
        "### Attempt 3 Support Vector Machines (Radial Kernel):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziwRR_rWYPtc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.feature_extraction import text\n",
        "import pandas as pd\n",
        "\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "\n",
        "ngram=4\n",
        "minf=0.05\n",
        "maxf=0.95\n",
        "stopwords = text.ENGLISH_STOP_WORDS\n",
        "\n",
        "NB_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(sublinear_tf=True,norm='l2',stop_words=stopwords,\n",
        "                                          strip_accents='ascii', lowercase=True, ngram_range=(1, ngram),\n",
        "                                          min_df=minf, max_df=maxf)),\n",
        "                ('clf', OneVsRestClassifier(svm.SVC(kernel='rbf',gamma='scale')))])\n",
        "\n",
        "total=0\n",
        "for category in LABELS:\n",
        "    print('... Processing {}'.format(category))\n",
        "    # train the model using X_dtm & y\n",
        "    NB_pipeline.fit(X_train, train[category])\n",
        "    # compute the testing accuracy\n",
        "    prediction = NB_pipeline.predict(X_test)\n",
        "    fpr, tpr, _ = roc_curve(test[category], prediction)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkLEeePJYPti"
      },
      "source": [
        "### Attempt 4 Multi Layer Perceptron:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Y4euk_cRYPtk"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "def create_model():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=(100), activation='tanh'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(512, activation='tanh'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1024, activation='tanh'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(512, activation='tanh'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256, activation='tanh'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(13, activation='sigmoid'))\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "Y_train = train[LABELS].values\n",
        "Y_test = test[LABELS].values\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"Report\"].values)\n",
        "\n",
        "print(X_train[0])\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=100, padding='post')\n",
        "                 \n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=100, padding='post')\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "model.fit(X_train, Y_train,batch_size=128,validation_split=0.2, epochs=10,verbose=1)\n",
        "\n",
        "\n",
        "total=0\n",
        "prediction = model.predict(X_test)\n",
        "for idx,category in enumerate(LABELS):\n",
        "    fpr, tpr, _ = roc_curve(Y_test[:, idx], prediction[:, idx])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxoykJcxYPtr"
      },
      "source": [
        "### Attempt 5 Convolution Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fgiT_pWYPts"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Input, Embedding\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "def create_model(voc):\n",
        "    sequence_input = Input(shape=(100,), dtype='int32')\n",
        "    embedded_sequences = Embedding(voc, 64, input_length=100)(sequence_input)\n",
        "    l_cov1= Conv1D(128, 3, activation='relu')(embedded_sequences)\n",
        "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "    l_cov2 = Conv1D(128, 3, activation='relu')(l_pool1)\n",
        "    l_pool2 = MaxPooling1D(17)(l_cov2)  # global max pooling\n",
        "    l_flat = Flatten()(l_pool2)\n",
        "    l_dense = Dense(128, activation='relu')(l_flat)\n",
        "    preds = Dense(13, activation='sigmoid')(l_dense)\n",
        "    model = Model(inputs=sequence_input, outputs=preds)\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "Y_train = train[LABELS].values\n",
        "Y_test = test[LABELS].values\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"Report\"].values)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=100, padding='post')\n",
        "voc_size = len(tokenizer.word_index)+1\n",
        "                 \n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=100, padding='post')\n",
        "\n",
        "print(voc_size)\n",
        "model = create_model(voc_size)\n",
        "\n",
        "model.fit(x=X_train, y=Y_train,batch_size=128,validation_split=0.2, epochs=10,verbose=1)\n",
        "\n",
        "\n",
        "total=0\n",
        "prediction = model.predict(X_test)\n",
        "for idx,category in enumerate(LABELS):\n",
        "    fpr, tpr, _ = roc_curve(Y_test[:, idx], prediction[:, idx])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbP-b2T-YPt0"
      },
      "source": [
        "# Attempt 6 Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OzitjU5IYPt3"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, LSTM, Bidirectional, Flatten, Input, Embedding\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "\n",
        "def create_model(voc):\n",
        "    sequence_input = Input(shape=(100,), dtype='int32')\n",
        "    embedded_sequences = Embedding(voc, 64, input_length=100)(sequence_input)\n",
        "    rnn_layer = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2,recurrent_dropout=0.2),\n",
        "                              merge_mode='concat')(embedded_sequences)\n",
        "    l_flat = Flatten()(rnn_layer)\n",
        "    l_dense = Dense(128, activation='relu')(l_flat)\n",
        "    preds = Dense(13, activation='sigmoid')(l_dense)\n",
        "    model = Model(inputs=sequence_input, outputs=preds)\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "Y_train = train[LABELS].values\n",
        "Y_test = test[LABELS].values\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"Report\"].values)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=100, padding='post')\n",
        "voc_size = len(tokenizer.word_index)+1\n",
        "                 \n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=100, padding='post')\n",
        "\n",
        "model = create_model(voc_size)\n",
        "\n",
        "model.fit(x=X_train, y=Y_train,batch_size=128,validation_split=0.2, epochs=10,verbose=1)\n",
        "\n",
        "\n",
        "total=0\n",
        "prediction = model.predict(X_test)\n",
        "for idx,category in enumerate(LABELS):\n",
        "    fpr, tpr, _ = roc_curve(Y_test[:, idx], prediction[:, idx])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1SkcQXYPt_"
      },
      "source": [
        "# Now its time for you to create you own model\n",
        "* Try to combine different Layers and different Parameters\n",
        "* Add other layers\n",
        "* The one with the best AUC value wins :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mrV5Q1lYPuB"
      },
      "source": [
        "## Your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBcmpFt4YPuD"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, LSTM, Bidirectional, Flatten, Input, Embedding,concatenate\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "MAXLEN=100  #Number of words of the reports used (be carefull if the value is too large)\n",
        "NUMBER_EPOCHS = 10 #Number of training iterations (be carefull if you train too much you will overfit the model)\n",
        "BATCH_SIZE = 128 #How many training examples we use before updating the network weights (too much will burn your pc :) )\n",
        "EMBEDDING_DIMENSION = 64 #The size of the vector that represents each word\n",
        "LSTM_UNITS=32 #Number of Units\n",
        "OPTIMIZER='adam' # 'sgd', 'rmsprop','adagrad','adadelta','adam','adamax','nadam'\n",
        "INTERMEDIATE_ACTIVATION='relu' #'tanh','relu',''\n",
        "\n",
        "def create_model(voc):\n",
        "    layer1 = Input(shape=(MAXLEN,), dtype='int32')\n",
        "    layer2 = Embedding(voc, EMBEDDING_DIMENSION, input_length=MAXLEN)(layer1)\n",
        "    \n",
        "    layer3a = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2,recurrent_dropout=0.2),\n",
        "                              merge_mode='concat')(layer2)\n",
        "    \n",
        "    layer3b = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.2,recurrent_dropout=0.2),\n",
        "                              merge_mode='concat')(layer2)\n",
        "    \n",
        "    layer3 = concatenate([layer3a,layer3b])\n",
        "    \n",
        "    layer4 = Flatten()(layer3)\n",
        "    \n",
        "    layer5 = Dense(128, activation='relu')(layer4)\n",
        "    layer6 = Dense(13, activation='sigmoid')(layer5)\n",
        "    \n",
        "    model = Model(inputs=layer1, outputs=layer6)\n",
        "    # Compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "df = pd.read_csv(\"dataset.csv\")\n",
        "train, test = train_test_split(df, random_state=42, test_size=0.33, shuffle=True)\n",
        "X_train = train[\"Report\"].values\n",
        "X_test = test[\"Report\"].values\n",
        "Y_train = train[LABELS].values\n",
        "Y_test = test[LABELS].values\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"Report\"].values)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=MAXLEN, padding='post')\n",
        "voc_size = len(tokenizer.word_index)+1\n",
        "                 \n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(X_test, maxlen=MAXLEN, padding='post')\n",
        "\n",
        "model = create_model(voc_size)\n",
        "\n",
        "model.fit(x=X_train, y=Y_train,batch_size=BATCH_SIZE,validation_split=0.2, epochs=NUMBER_EPOCHS,verbose=1)\n",
        "\n",
        "\n",
        "LABELS = ['No findings','Cardiomegaly','Airspace Opacity',\n",
        "             'Lung Lesion','Edema','Consolidation','Pneumonia','Atelectasis',\n",
        "             'Pneumothorax','Pleural Effusion','Pleural Other','Fracture','Support Devices']\n",
        "total=0\n",
        "prediction = model.predict(X_test)\n",
        "for idx,category in enumerate(LABELS):\n",
        "    fpr, tpr, _ = roc_curve(Y_test[:, idx], prediction[:, idx])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    total+=roc_auc\n",
        "    print('Test auc is {}'.format(roc_auc))\n",
        "\n",
        "print(\"Macro Average AUC:\" + str(total/13))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYNfxrXIYPuQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}